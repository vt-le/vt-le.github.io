
<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Viet-Tuan Le | publications</title>
<meta name="description" content="Viet-Tuan Le's website.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="css/main.css">

<link rel="canonical" href="publication">

<!-- Theming-->


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Viet-Tuan Le</span>
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="index.html">
              About
              
            </a>
          </li>
          
          
          
          <!-- Other pages -->
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="publications.html">
                Publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>

          <li class="nav-item ">
              <a class="nav-link" href="#">
                CV                
              </a>
          </li>

          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="#">
              blog
              
            </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
    <!-- <p class="post-description">(&#x2A) denotes equal contribution.</p>-->
  </header>

<article>
    <!-- An up-to-date list is available on <a href="https://scholar.google.com/citations?user=VdlgOXoAAAAJ&hl=en" target="\_blank">Google Scholar</a> -->
<div class="publications">

  <h2 class="year">2023</h2>
  <ol class="bibliography">


    <li><div class="row">
      <div class="col-sm-2 abbr">
        <abbr class="badge">Journal</abbr>
      </div>
    
      <div id="le2022anomaly" class="col-sm-8">
        
          <div class="title">GoP: Guided Motion Prediction for Video Anomaly Detection</div>
          <div class="author">
                <em>Viet-Tuan Le</em> and <a href="http://home.sejong.ac.kr/~ykim/">Yong-Guk Kim</a>
            
          </div>
       <div class="periodical">    
            <em><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">IEEE Transactions on Circuits and Systems for Video Technology</a></em>, 2023
          </div>  
    
        <div class="links">
          <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          <a href="#" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
          <a href="https://vt-le.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
          <a href="https://github.com/vt-le/" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
        </div>
    
        <!-- Hidden abstract block -->
        <div class="abstract hidden">
          <p>...</p>
        </div>
      </div>
    </div>
    </li>


 <li><div class="row">
  <div class="col-sm-2 abbr">
    <abbr class="badge">Journal</abbr>
  </div>

  <div id="le2022hstforu" class="col-sm-8">
    
      <div class="title">HSTforU: Anomaly Detection in Aerial and Ground-based Videos with Hierarchical Spatio-Temporal Transformer for U-net</div>
      <div class="author">
                <em>Viet-Tuan Le</em>, Hulin Jin, and <a href="http://home.sejong.ac.kr/~ykim/">Yong-Guk Kim</a>
        
      </div>
   <div class="periodical">    
      <em><a href="https://www.sciencedirect.com/journal/expert-systems-with-applications">Expert Systems with Applications</a></em>, 2023
      </div>  

    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="#" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      <a href="https://vt-le.github.io/HSTforU/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
      <a href="https://github.com/vt-le/HSTforU" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Anomaly detection is to identify abnormal events against normal ones within surveillance videos mainly collected in ground-based settings. Recently, the need for drone data processing is rapidly growing as drones find new applications. However, as most aerial videos collected by flying drones contain moving backgrounds and others, it is necessary to deal with their spatio-temporal features in detecting anomalies. This study presents a transformer-based video anomaly detection method whereby we investigate a challenging aerial dataset and three benchmark ground-based datasets. The encoder of our U-net has a four-stage pyramid transformer structure, and each stage has a link to a corresponding spatio-temporal transformer stage. Then, this transformer produces hierarchical feature maps that are conveyed to the decoder as skip connections. Extensive evaluations including several ablation studies suggest that this network outperforms the state-of-the-art on Drone-anomaly dataset and three benchmark datasets. We expect the proposed transformer for U-net can find diverse applications in the video processing area. Code and model are available at https://vt-le.github.io/HSTforU/.</p>
    </div>
  </div>
</div>
</li>



<h2 class="year">2022</h2>
<ol class="bibliography">

  <li>
    <div class="row">
  <div class="col-sm-2 abbr">
    <abbr class="badge">Journal</abbr>
  </div>

  <div id="le2022attention" class="col-sm-8">
    
      <div class="title">Attention-based Residual Autoencoder for Video Anomaly Detection</div>
      <div class="author">
                <em>Viet-Tuan Le</em> and <a href="http://home.sejong.ac.kr/~ykim/">Yong-Guk Kim</a>
        
      </div>
   <div class="periodical">    
        <em><a href="https://www.springer.com/journal/10489">Applied Intelligence</a></em>, 2022
      </div>  

    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://link.springer.com/article/10.1007/s10489-022-03613-1" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      <a href="https://vt-le.github.io/astnet/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
      <a href="https://github.com/vt-le/astnet" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Automatic anomaly detection is a crucial task in video surveillance system intensively used for public safety and others. The present system adopts a spatial branch and a temporal branch in a unified network that exploits both spatial and temporal information effectively. The network has a residual autoencoder architecture, consisting of a deep convolutional neural network-based encoder and a multi-stage channel attention-based decoder, trained in an unsupervised manner. The temporal shift method is used for exploiting the temporal feature, whereas the contextual dependency is extracted by channel attention modules. System performance is evaluated using three standard benchmark datasets. Result suggests that our network outperforms the state-of-the-art methods, achieving 97.4% for UCSD Ped2, 86.7% for CUHK Avenue, and 73.6% for ShanghaiTech dataset in term of Area Under Curve, respectively.</p>
    </div>
  </div>
    </div>
  </li>
</ol>

 

<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Journal</abbr>
  </div>

  <div id="le2019a" class="col-sm-8">
    
      <div class="title">A comprehensive review of recent deep learning techniques for human activity recognition</div>
      <div class="author">
                <em>Viet-Tuan Le</em>, Kiet Tran-Trung, and <a href="https://scholar.google.com/citations?user=5E3HHtsAAAAJ&hl=en" target="_blank">Vinh Truong Hoang</a>
        
      </div>
   <div class="periodical">
      
        <em>Computational Intelligence and Neuroscience</em>, 2022
  
      </div>  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://downloads.hindawi.com/journals/cin/2022/8323962.pdf?_gl=1*ye8tr8*_ga*MTgyMzU1MDMxNS4xNjg5ODI4Njk5*_ga_NF5QFMJT5V*MTY4OTgyODY5OS4xLjAuMTY4OTgyODY5OS42MC4wLjA.&_ga=2.129955726.301264949.1689828700-1823550315.1689828699" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>      
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Human action recognition is an important field in computer vision that has attracted remarkable attention from researchers. This survey aims to provide a comprehensive overview of recent human action recognition approaches based on deep learning using RGB video data. Our work divides recent deep learning-based methods into five different categories to provide a comprehensive overview for researchers who are interested in this field of computer vision. Moreover, a pure-transformer architecture (convolution-free) has outperformed its convolutional counterparts in many fields of computer vision recently. Our work also provides recent convolution-free-based methods which replaced convolution networks with the transformer networks that achieved state-of-the-art results on many human action recognition datasets. Firstly, we discuss proposed methods based on a 2D convolutional neural network. Then, methods based on a recurrent neural network which is used to capture motion information are discussed. 3D convolutional neural network-based methods are used in many recent approaches to capture both spatial and temporal information in videos. However, with long action videos, multistream approaches with different streams to encode different features are reviewed. We also compare the performance of recently proposed methods on four popular benchmark datasets. We review 26 benchmark datasets for human action recognition. Some potential research directions are discussed to conclude this survey.</p>
    </div>
    
  </div>
</div>
</li>

  <h2 class="year">2019</h2>
  <ol class="bibliography">

 <li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICSPS</abbr>
    <span class="award badge">Oral</span>
  </div>

  <div id="tuan2012local" class="col-sm-8">
    
      <div class="title">Local binary pattern based on image gradient for bark image classification</div>
      <div class="author">
                <em>Tuan Le-Viet</em> and <a href="https://scholar.google.com/citations?user=5E3HHtsAAAAJ&hl=en" target="_blank">Vinh Truong Hoang</a>
        
      </div>
   <div class="periodical">
      
        <em>Tenth International Conference on Signal Processing Systems (ICSPS), Singapore</em>,
        2019
      
      </div>  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://doi.org/10.1117/12.2522093" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>In this work, we present a discriminative and effective local texture descriptor for bark image classification. The proposed descriptor is based on three factors, namely, pixel, magnitude and direction value. Unlike most other descriptors based on original local binary pattern, the proposed descriptor is conducted the changing of local texture of bark image. The performance of the proposed descriptor is evaluated on three benchmark datasets. The experimental results show that our approach is highly effective.</p>
    </div>
    
  </div>
</div>
</li> 
</ol>


<h2 class="year">2012</h2>
  <ol class="bibliography">

 <li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICCAIS</abbr>    
  </div>

  <div id="tuan2012human" class="col-sm-8">
    
      <div class="title">Human action recognition on simple and complex background in video</div>
      <div class="author">
                <em>Tuan Le-Viet</em> and <a href="https://scholar.google.com/citations?hl=en&user=MskoD4gAAAAJ" target="_blank">Ngoc Quoc Ly</a>
        
      </div>
   <div class="periodical">
      
        <em>International Conference on Control, Automation and Information Sciences</em>,
        2012
      
      </div>  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://ieeexplore.ieee.org/document/6466569" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>In this paper, we propose a new method for human action recognition on simple and complex background. There are two approaches: (a) for homogeneous and static background, we propose a combination of HOG3D and HOF to encode the appearance and motion information of human. (b) with realistic video data (including changes of viewpoint, scale, and lighting conditions, partial occlusion of humans and objects, cluttered backgrounds), we exploit the co-occurrence between scene and actions by combining HOG3D and Opponent SIFT to encode action (how) and scene (where) information. The first combination is applied in surveillance videos or videos in which camera placement and parameters are fixed and known. The second one is applied to remained video in which background is not static and cluttered. The experimental results have shown the efficiency and effectiveness of our method.</p>
    </div>
    
  </div>
</div>
</li> 
</ol>




</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    

  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="./js/mansory.js" type="text/javascript"></script>


  


<!-- Global site tag (gtag.js) - Google Analytics -->



<!-- Load Common JS -->
<script src="./js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="./js/dark_mode.js"></script>


</html>
